
梯度提升算法
Freidman提出了梯度提升算法，该方法是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值
−[∂L(y,f(xi))∂f(xi)]f(x)=fm−1(x)
作为回归问题算法中的残差的近似值，拟合一个回归模型。
其算法流程如下：
F0(x)=argminρ∑Ni=1L(yi,ρ)
For m=1 to M do:
ỹ i=−[∂L(y,F(xi))∂F(xi)]F(x)=Fm−1(x),i=1,N
am=argmina,β∑Ni=1[ỹ i−βh(xi;a)]2
ρm=argminρ∑Ni=1L(yi,Fm−1(xi)+ρh(xi;am))
Fm(x)=Fm−1(x)+ρmh(x;am)
endFor
endAlgorighm
其中h(xi;am)表示基本分类器（weak learner or base learner），4中am表示拟合负梯度能力最好的分类器参数
负梯度只是表示下降的方向，但是下降多少没有确定，5中ρm可以认为是下降最快的步长，可以让Loss最小，可以用线性搜索的方式来估计ρm的值
为何这里不直接利用负梯度来调节，而是需要用一个分类器来拟合呢？因为这里的负梯度是在训练集上求出的，不能被泛化测试集中。我们的参数是在一个函数空间里面，不能使用例如SGD这样的求解方式。使用一个分类器来拟合，是一个泛化的方式。
回归树
当我们的基本分类器是一个包含J个节点的回归树时，回归树模型可以表示为
h(x;{bj,Rj}J1)=∑b=jJbjI(x∈Rj)(8)
其中{Rj}J1不相交的区域，它们的集合覆盖了预测值的空间，{bj}J1是叶子节点的值，可以认为是模型h的系数
利用回归树模型，算法流程6中的公式可以被替换为：
Fm(x)=Fm−1(x)+ρm∑j=1JbjmI(x∈Rjm)(9)
其中{Rjm}J1是第m次迭代生成的树所产生的区域。第m次迭代的树用来预测流程3中由流程4中平方误差产生的{ỹ i}Ni
{bjm}可以被表示为
bjm=avexi∈Rjmỹ i
即用平均值表示该叶子节点拟合的值
有了下降的方向，我们还需要最好的步长，缩放因子ρm是流程5中线性搜索方式的一种解决方案
从上面可以看出，我们是先求的bjm，然后在求解ρm，我们能否同时求解呢？
另γjm=ρmbjm，公式9可以被表示为：
Fm(x)=Fm−1(x)+∑j=1JγjmI(x∈Rjm)(10)
通过优化如下公式来获取最优的系数γjm：
{γjm}J1=argmin γj J1∑i=1NL(yi,Fm−1(xi)+∑j=1JγjI(x∈Rjm))(11)
由于回归树产生的叶子节点各个区域之间是不相交的，且所有的样本最终都会属于某个叶子节点，所以公式11可以表示为：
γjm=argminγ∑xi∈RjmL(yi,Fm−1(xi)+γ)
给定当前Fm−1(xi)，γjm可以作为叶子节点的值，该值可以看做是基于损失函数L的每个叶子节点的最理想的常数更新值，也可以认为γjm是即有下降方向又有下降步长的值。
综上，用回归树作为基本分类器的梯度提升算法流程可以如下表示：
F0(x)=argminρ∑Ni=1L(yi,ρ)
For m=1 to M do:
ỹ i=−[∂L(y,F(xi))∂F(xi)]F(x)=Fm−1(x),i=1,N
{Rjm}J1=J−terminalnodetree({ỹ i,xi}Ni)
γjm=argminγ∑xi∈RjmL(yi,Fm−1(xi)+γ)
Fm(x)=Fm−1(x)+∑Jj=1γjmI(x∈Rjm)
endFor
endAlgorighm
其中3是计算残差（利用损失函数的负梯度在当前模型的值作为残差的近似值），4是拟合一颗含有J个叶子节点的回归树，5是估计回归树叶子节点的值
下面我们看一下二元分类、多元分类、回归中残差的计算、叶子节点值的估计。
Two-class logistic regression and classification
我们用negative binomial log-likehood作为我们的损失函数：
L(y,F)=log(1+exp(−2yF)),y∈−1,1(12)
其中
F(x)=12log[Pr(y=1|x)Pr(y=−1|x)](13)
公式13是logit函数，log odds
如上公式是Freidman的论文中使用的公式，我认为使用在逻辑回归中常见的L(y,F)=ylogF+(1−y)log(1−F)，其中F(z)=11+exp(−z)也可以
计算残差：
ỹ i=−[∂L(y,F(xi))∂F(xi)]F(x)=Fm−1(x)=2yi1+exp(2yiFm−1(xi))(14)
叶子节点值的估计：
γjm=argminγ∑xi∈Rjmlog(1+exp(−2yi(Fm−1(xi)+γ)))(15)
可以通过一步Newton-Raphson来近似公式15，估计结果为：
γjm=∑xi∈Rjmỹ i∑xi∈Rjm|ỹ i|(2−|ỹ i|)
最终得到的FM(x)与对数几率 log-odds相关，我们可以用来进行概率估计
F(x)=12log(p1−p)
e2F(x)=p(1−p)
P+(x)=p=e2F(x)1+e2F(x)=11+e−2F(x)
P−(x)=1−p=11+e2F(x)
有了概率之后，我们接下来就可以利用概率进行分类
Multi-class logistic regression and classification
我们使用multi-class log-loss作为损失函数：
L({yk,Fk(x)}K1)=−∑k=1Kyklogpk(x)(16)
其中使用softmax来计算概率：
pk(x)=exp(Fk(x))/∑l=1Kexp(Fl(x))(17)
从公式17可以得出，对于多分类问题，我们需要为每个类别创建一颗回归树Fl(x)l=1,2,...,k
计算残差：
ỹ ik=−[∂L({yil,Fl(xi)}Kl=1)∂Fk(xi)]{Fl(x)=Fl,m−1(x)}K1=yik−pk,m−1(xi)(18)
我们假定共分为3类，那么logloss为：
L=−y1logexp(F1(x))exp(F1(x))+exp(F1(x))+exp(F1(x))−y2logexp(F2(x))exp(F1(x))+exp(F1(x))+exp(F1(x))−y3logexp(F3(x))exp(F1(x))+exp(F1(x))+exp(F1(x))
∂L∂F1(x)=−y1+y1p1+y2p2+y3p3
∂L∂F2(x)=y1p1−y2+y2p2+y3p3
∂L∂F3(x)=y1p1+y2p2−y3+y3p3
如果当期样本的类别为(1,0,0)，那么
∂L∂F1(x)=−1+p1
∂L∂F2(x)=p2
∂L∂F3(x)=p3
取负梯度，则
−∂L∂F1(x)=1−p1
−∂L∂F2(x)=−p2=0−p2
−∂L∂F3(x)=−p3=0−p3
符合公式18中的ỹ ik=yik−pk,m−1(xi)
叶子节点值的估计：
{rjkm}=argminγjk∑i=1N∑k=1Kϕ(yik,Fk,m−1(xi)+∑j=1JγjkI(xi∈Rjm)})(19)
可以通过一步Newton-Raphson来近似公式19，估计结果为：
γjkm=K−1K∑xi∈Rjkmỹ ik∑xi∈Rjkm|ỹ ik|(1−|ỹ ik|)
Regression
我们使用Least-squares作为损失函数：
L(y,F)=(y−F)22
计算残差：
ỹ i=−[∂L(y,F(xi))∂F(xi)]F(x)=Fm−1(x)=yi−Fm−1(xi)(20)
叶子节点值的估计：
γjm=argminγ∑xi∈Rjm12(yi−(Fm−1(xi)+γ))2(21)
γjm=argminγ∑xi∈Rjm12(yi−Fm−1(xi)−γ)2
γjm=argminγ∑xi∈Rjm12(ỹ i−γ)2
容易得出以下结果：
γjm=avexi∈Rjmỹ i
回归树的创建
拟合残数是一个回归问题，所以在分割样本时，我们不会采用基尼指数（Gini）、信息增益（IG）等用于分类的标准。
我们可以选用MSE(mean square error impurity criterion)作为分割样本的标准。 也可是采用Friedman在论文中的the least-squares improvement criterion，公式如下：
i2(Rl,Rr)=wlwrwl+wr(y¯l−y¯r)2
其中y¯ly¯r分别是左右孩子的平均值，wlwr分别是左右孩子对应的权重和
本文是针对具体的损失函数进行的相关推导，泛化能力差，大家可以参考xgboost作者的这篇文章，作者进行了更加一般的推导，这一个抽象的形式对于实现机器学习工具也是非常有帮助的。
引用：
Greedy Function Approximation: A Gradient Boosting Machine